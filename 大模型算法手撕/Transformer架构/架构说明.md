# Transformer 架构详解

## 整体架构

```
输入序列 (Source)                    输入序列 (Target)
      ↓                                    ↓
   Embedding                           Embedding
      ↓                                    ↓
Positional Encoding              Positional Encoding
      ↓                                    ↓
   ┌──────────┐                       ┌──────────┐
   │ Encoder  │                       │ Decoder  │
   │  Stack   │ ─────────────────────→│  Stack   │
   │  (N层)   │   Encoder Output      │  (N层)   │
   └──────────┘                       └──────────┘
                                           ↓
                                     Linear Layer
                                           ↓
                                        Softmax
                                           ↓
                                      输出概率分布
```

## 编码器 (Encoder) 结构

每个编码器层包含两个子层：

```
输入 (x)
   ↓
┌─────────────────────────────────┐
│  Multi-Head Self-Attention      │  ← 自注意力机制
└─────────────────────────────────┘
   ↓ (残差连接 + 层归一化)
┌─────────────────────────────────┐
│  Feed-Forward Network           │  ← 前馈神经网络
└─────────────────────────────────┘
   ↓ (残差连接 + 层归一化)
输出
```

### 编码器特点：
- **自注意力机制**: Q、K、V 都来自输入序列本身
- **无掩码**: 可以看到整个输入序列
- **并行计算**: 所有位置可以同时处理

## 解码器 (Decoder) 结构

每个解码器层包含三个子层：

```
输入 (x)
   ↓
┌─────────────────────────────────┐
│  Masked Multi-Head              │  ← 带掩码的自注意力
│  Self-Attention                 │     (防止看到未来)
└─────────────────────────────────┘
   ↓ (残差连接 + 层归一化)
┌─────────────────────────────────┐
│  Multi-Head Cross-Attention     │  ← 交叉注意力
│  (Encoder-Decoder Attention)    │     (关注编码器输出)
└─────────────────────────────────┘
   ↓ (残差连接 + 层归一化)
┌─────────────────────────────────┐
│  Feed-Forward Network           │  ← 前馈神经网络
└─────────────────────────────────┘
   ↓ (残差连接 + 层归一化)
输出
```

### 解码器特点：
- **Masked Self-Attention**: 只能看到当前位置之前的信息
- **Cross-Attention**: 关注编码器的输出
- **自回归**: 逐步生成输出序列

## 注意力机制详解

### 1. 缩放点积注意力 (Scaled Dot-Product Attention)

```
Query (Q)  Key (K)  Value (V)
   ↓         ↓         ↓
   └────QK^T─┘         │
        ↓              │
   ÷ √d_k             │
        ↓              │
   Softmax            │
        ↓              │
        └──────────────┘
              ↓
         Attention(Q,K,V)
```

**公式**: 
```
Attention(Q, K, V) = softmax(QK^T / √d_k)V
```

**参数说明**:
- Q (Query): 查询向量
- K (Key): 键向量
- V (Value): 值向量
- d_k: Key 的维度
- √d_k: 缩放因子（防止点积过大）

### 2. 多头注意力 (Multi-Head Attention)

```
输入 (x)
   ↓
   ├─→ Linear (W_Q) ─→ Q₁ ─┐
   ├─→ Linear (W_K) ─→ K₁ ─┤
   └─→ Linear (W_V) ─→ V₁ ─┤
                            ├─→ Attention₁ ─┐
   ├─→ Linear (W_Q) ─→ Q₂ ─┐                │
   ├─→ Linear (W_K) ─→ K₂ ─┤                │
   └─→ Linear (W_V) ─→ V₂ ─┤                │
                            ├─→ Attention₂ ─┤
        ... (重复 h 次) ...                  │
                                             ├─→ Concat
   ├─→ Linear (W_Q) ─→ Qₕ ─┐                │
   ├─→ Linear (W_K) ─→ Kₕ ─┤                │
   └─→ Linear (W_V) ─→ Vₕ ─┤                │
                            ├─→ Attentionₕ ─┘
                                             ↓
                                        Linear (W_O)
                                             ↓
                                          输出
```

**特点**:
- 将 d_model 维度分成 h 个头
- 每个头的维度: d_k = d_v = d_model / h
- 允许模型关注不同位置的不同表示子空间

## 前馈神经网络 (Feed-Forward Network)

```
输入 (x)
   ↓
Linear(d_model → d_ff)
   ↓
ReLU
   ↓
Dropout
   ↓
Linear(d_ff → d_model)
   ↓
Dropout
   ↓
输出
```

**公式**:
```
FFN(x) = max(0, xW₁ + b₁)W₂ + b₂
```

**参数**:
- d_model: 输入/输出维度 (默认 512)
- d_ff: 隐藏层维度 (默认 2048)

## 位置编码 (Positional Encoding)

由于 Transformer 没有循环或卷积结构，需要显式添加位置信息。

**公式**:
```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

**特点**:
- pos: 位置索引
- i: 维度索引
- 使用正弦和余弦函数的不同频率
- 固定的位置编码（不需要训练）

## 残差连接和层归一化

每个子层都使用残差连接和层归一化：

```
输入 (x)
   │
   ├────────────┐
   ↓            │
子层操作         │
   ↓            │
Dropout         │
   ↓            │
   +←───────────┘ (残差连接)
   ↓
LayerNorm
   ↓
输出
```

**公式**:
```
output = LayerNorm(x + Sublayer(x))
```

**好处**:
- 缓解梯度消失问题
- 加速训练
- 提高模型性能

## 掩码 (Masking)

### 1. 填充掩码 (Padding Mask)
- 用于屏蔽填充位置
- 应用于编码器和解码器

### 2. 前瞻掩码 (Look-ahead Mask)
- 防止解码器看到未来的信息
- 只应用于解码器的自注意力

```
前瞻掩码示例 (序列长度=5):
[0, -∞, -∞, -∞, -∞]
[0,  0, -∞, -∞, -∞]
[0,  0,  0, -∞, -∞]
[0,  0,  0,  0, -∞]
[0,  0,  0,  0,  0]
```

## 数据流示例

### 训练时的完整流程：

1. **输入处理**
   ```
   源序列: "I love AI" → [1, 23, 456] → Embedding + PE
   目标序列: "我爱人工智能" → [2, 34, 567, 890] → Embedding + PE
   ```

2. **编码**
   ```
   源序列 → Encoder Stack → Encoder Output
   ```

3. **解码**
   ```
   目标序列 + Encoder Output → Decoder Stack → Output Logits
   ```

4. **输出**
   ```
   Output Logits → Softmax → 概率分布 → 预测词
   ```

### 推理时的流程：

1. **编码阶段**
   ```
   源序列 → Encoder → Encoder Output
   ```

2. **解码阶段** (自回归)
   ```
   [<START>] → Decoder → 预测第1个词
   [<START>, word₁] → Decoder → 预测第2个词
   [<START>, word₁, word₂] → Decoder → 预测第3个词
   ...
   直到生成 <END> 或达到最大长度
   ```

## 关键超参数

| 参数 | 论文中的值 | 说明 |
|------|-----------|------|
| d_model | 512 | 模型维度 |
| n_layers | 6 | 编码器/解码器层数 |
| n_heads | 8 | 注意力头数 |
| d_ff | 2048 | 前馈网络维度 |
| dropout | 0.1 | Dropout 概率 |
| max_len | 5000 | 最大序列长度 |

## 计算复杂度

| 层类型 | 每层复杂度 | 序列操作数 | 最大路径长度 |
|--------|-----------|-----------|-------------|
| Self-Attention | O(n²·d) | O(1) | O(1) |
| Recurrent | O(n·d²) | O(n) | O(n) |
| Convolutional | O(k·n·d²) | O(1) | O(log_k(n)) |

**说明**:
- n: 序列长度
- d: 表示维度
- k: 卷积核大小

## 训练技巧

### 1. 学习率预热 (Warmup)
```
lrate = d_model^(-0.5) · min(step^(-0.5), step · warmup^(-1.5))
```

### 2. 标签平滑 (Label Smoothing)
- 防止模型过度自信
- 提高泛化能力

### 3. Dropout
- 应用于注意力权重
- 应用于每个子层的输出

### 4. 参数初始化
- 使用 Xavier 均匀初始化

## 优势与局限

### ✅ 优势
- 并行计算效率高
- 能捕捉长距离依赖
- 可解释性好（注意力可视化）
- 训练速度快

### ❌ 局限
- 对位置编码的依赖
- 序列长度的二次复杂度
- 需要大量数据
- 计算资源要求高

## 后续发展

基于 Transformer 的改进模型：
- **BERT**: 双向编码器
- **GPT**: 单向解码器
- **T5**: 统一的 Seq2Seq 框架
- **BART**: 结合 BERT 和 GPT
- **Transformer-XL**: 处理更长序列
- **Reformer**: 降低复杂度

